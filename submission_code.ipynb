{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name = \"allenai/unifiedqa-v2-t5-large-1251000\" # you can specify the model size here\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def run_model(input_string, **generator_args):\n",
    "    input_ids = tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = model.generate(input_ids, **generator_args)\n",
    "    return tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "\n",
    "def run_gpt(input_string):\n",
    "    output = openai.Completion.create(engine=\"text-davinci-002\",\n",
    "                                                       prompt=input_string,\n",
    "                                                       max_tokens=256,\n",
    "                                                       temperature=0,\n",
    "                                                       top_p=0)['choices'][0]['text']\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_questions = json.load(open('autocast_questions.json')) # from the Autocast dataset\n",
    "test_questions = json.load(open('autocast_competition_test_set.json'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline models outputting random answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        return np.random.random(size=2)\n",
    "    elif question['qtype'] == 'mc':\n",
    "        probs = np.random.random(size=len(question['choices']))\n",
    "        return probs / probs.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return np.random.random()\n",
    "\n",
    "\n",
    "def calibrated_random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        question_str = question['question'] + ' (a) true (b) false. Don\\' give explanation.'\n",
    "        ans = run_gpt(question_str)\n",
    "        pred = np.ones(2)\n",
    "        if (ans == '\\n\\nfalse'):\n",
    "            pred[1]+=1e-5\n",
    "        else :\n",
    "            pred[0]+=1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'mc':\n",
    "        if(len(question['choices'])) > 26:\n",
    "            pred = np.ones(len(question['choices']))\n",
    "            return pred / pred.sum()\n",
    "        print(question['choices'])\n",
    "        question_str = question['question']\n",
    "        choice_letters = [\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"K\", \"L\", \"M\", \"N\", \"O\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"U\", \"V\", \"W\", \"X\", \"Y\", \"Z\"]\n",
    "        for i, choice in enumerate(question['choices']):\n",
    "            question_str += \"(\" + f\"{choice_letters[i]})  {choice} \"\n",
    "        question_str += '. Only give the index or your answer.'\n",
    "        ans = run_gpt(question_str)\n",
    "        \n",
    "        pred = np.ones(len(question['choices']))\n",
    "\n",
    "        # lowercase_choices = [string.lower() for string in question['choices']]\n",
    "        for i, choice in enumerate(choice_letters):\n",
    "            if (i > len(question['choices'])):\n",
    "                break\n",
    "            if (choice in ans):\n",
    "                pred[i] +=1e-5\n",
    "                break\n",
    "        if (pred.sum() == 0) :\n",
    "            pred = np.ones(len(question['choices']))\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.4\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    if question['qtype'] == 't/f':\n",
    "        pred_idx = np.argmax(np.random.random(size=2))\n",
    "        pred = np.ones(2)\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        pred = np.ones(len(question['choices']))\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.4\n",
    "    \n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Less than 6.30', 'Between 6.30 and 6.35, inclusive', 'More than 6.35 but less than 6.40', '6.40 or more']\n",
      "['A majority', 'A plurality', 'Not a plurality']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/Users/ricardoli/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/Users/ricardoli/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[192], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mif\u001b[39;00m question[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# skipping questions without answer\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m preds\u001b[39m.\u001b[39mappend(calibrated_random_baseline_model(question))\n\u001b[1;32m     10\u001b[0m \u001b[39mif\u001b[39;00m question[\u001b[39m'\u001b[39m\u001b[39mqtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt/f\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     11\u001b[0m     ans_idx \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m question[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[191], line 15\u001b[0m, in \u001b[0;36mcalibrated_random_baseline_model\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     13\u001b[0m question_str \u001b[39m=\u001b[39m question[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m (a) true (b) false. Don\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m give explanation.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     14\u001b[0m ans \u001b[39m=\u001b[39m run_gpt(question_str)\n\u001b[0;32m---> 15\u001b[0m pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m (ans \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     17\u001b[0m     pred[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[191], line 15\u001b[0m, in \u001b[0;36mcalibrated_random_baseline_model\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     13\u001b[0m question_str \u001b[39m=\u001b[39m question[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m (a) true (b) false. Don\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m give explanation.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     14\u001b[0m ans \u001b[39m=\u001b[39m run_gpt(question_str)\n\u001b[0;32m---> 15\u001b[0m pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mif\u001b[39;00m (ans \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     17\u001b[0m     pred[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1087\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1078\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "for question in autocast_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    preds.append(calibrated_random_baseline_model(question))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 25.00, MCQ: 38.05, NUM: 21.22\n",
      "Combined Metric: 84.27\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fewer than 2.2 million', 'Between 2.2 million and 2.8 million, inclusive', 'More than 2.8 million but fewer than 3.4 million', 'Between 3.4 million and 4.0 million, inclusive', 'More than 4.0 million']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"/Users/ricardoli/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"/Users/ricardoli/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[207], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m question \u001b[39min\u001b[39;00m test_questions:\n\u001b[1;32m      4\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     preds\u001b[39m.\u001b[39mappend(calibrated_random_baseline_model(question))\n",
      "Cell \u001b[0;32mIn[206], line 14\u001b[0m, in \u001b[0;36mcalibrated_random_baseline_model\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mif\u001b[39;00m question[\u001b[39m'\u001b[39m\u001b[39mqtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mt/f\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     13\u001b[0m     question_str \u001b[39m=\u001b[39m question[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m (a) true (b) false. Don\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m give explanation.\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m     ans \u001b[39m=\u001b[39m run_gpt(question_str)\n\u001b[1;32m     15\u001b[0m     pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[39mif\u001b[39;00m (ans \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mfalse\u001b[39m\u001b[39m'\u001b[39m):\n",
      "Cell \u001b[0;32mIn[181], line 21\u001b[0m, in \u001b[0;36mrun_gpt\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_gpt\u001b[39m(input_string):\n\u001b[0;32m---> 21\u001b[0m     output \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mCompletion\u001b[39m.\u001b[39mcreate(engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-002\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                                                        prompt\u001b[39m=\u001b[39minput_string,\n\u001b[1;32m     23\u001b[0m                                                        max_tokens\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                                        temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                                        top_p\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "Cell \u001b[0;32mIn[181], line 21\u001b[0m, in \u001b[0;36mrun_gpt\u001b[0;34m(input_string)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_gpt\u001b[39m(input_string):\n\u001b[0;32m---> 21\u001b[0m     output \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39mCompletion\u001b[39m.\u001b[39mcreate(engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-davinci-002\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m                                                        prompt\u001b[39m=\u001b[39minput_string,\n\u001b[1;32m     23\u001b[0m                                                        max_tokens\u001b[39m=\u001b[39m\u001b[39m256\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                                        temperature\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     25\u001b[0m                                                        top_p\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1087\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1078\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_gpu/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "preds = []\n",
    "for question in test_questions:\n",
    "    time.sleep(2)\n",
    "    preds.append(calibrated_random_baseline_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1364\n",
      "updating: predictions.pkl (deflated 78%)\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('submission'):\n",
    "    os.makedirs('submission')\n",
    "\n",
    "print(len(preds))\n",
    "with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "929\n"
     ]
    }
   ],
   "source": [
    "print(len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_CS506",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "181092291ceb3bde47f780fe59d5889789b12e4f4a2724d4d08096132374e669"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
